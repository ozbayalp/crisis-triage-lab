version: "3.9"

services:
  # ===========================================
  # Backend API (FastAPI)
  # ===========================================
  backend:
    build:
      context: .
      dockerfile: infra/docker/Dockerfile.backend
    ports:
      - "8000:8000"
    environment:
      - APP_ENV=development
      - REDIS_URL=redis://redis:6379/0
      - TRANSCRIPTION_BACKEND=whisper
      - TRIAGE_MODEL_BACKEND=neural
      - NEURAL_MODEL_DIR=/app/models/neural_triage_v1/best_model
      - ALLOWED_ORIGINS=*
    volumes:
      - ./backend:/app/backend
      - ./ml/src:/app/ml/src  # Shared ML code
      - ./ml/models:/app/models  # Model weights (extract first!)
    depends_on:
      - redis
    working_dir: /app/backend
    command: uvicorn main:app --host 0.0.0.0 --port 8000 --reload

  # ===========================================
  # Frontend (Next.js)
  # ===========================================
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    ports:
      - "3000:3000"
    environment:
      - NEXT_PUBLIC_BACKEND_HTTP_URL=http://backend:8000
      - NEXT_PUBLIC_BACKEND_WS_URL=ws://backend:8000
    depends_on:
      - backend

  # ===========================================
  # Redis (session state, optional persistence)
  # ===========================================
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly no  # Ephemeral by default

volumes:
  redis_data:

# ===========================================
# Notes:
# - IMPORTANT: Extract the model before running:
#   cd ml/models/neural_triage_v1
#   zip -s 0 best_model.zip --out combined.zip && unzip combined.zip
#
# - For GPU support, use nvidia-docker and add:
#   deploy:
#     resources:
#       reservations:
#         devices:
#           - capabilities: [gpu]
# ===========================================
